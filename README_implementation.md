# CoNLL-U Data Processing for PoS Tagging - Part 1 Implementation

This repository contains the implementation of data processing functions for the first part of your Natural Language Understanding PoS tagging assignment.

## ğŸ¯ Assignment Completion Status

âœ… **PART 1 COMPLETED**: Functions to process the UD English treebank have been successfully implemented.

### What was implemented:

- âœ… CoNLL-U file parser that handles the Universal Dependencies format
- âœ… Multiword token removal (e.g., "don't" â†’ "do" + "n't")
- âœ… Empty node removal (decimal IDs like "10.1")
- âœ… Sentence length filtering (max 128 words)
- âœ… Clean data extraction as (word, UPOS_tag) pairs
- âœ… Comprehensive validation and statistics
- âœ… Full test suite with validation

## ğŸ“Š Data Processing Results

UD English treebank data has been successfully processed:

| Dataset         | Original Sentences | Kept Sentences | Tokens  | Multiword Removed | Long Sentences Removed |
| --------------- | ------------------ | -------------- | ------- | ----------------- | ---------------------- |
| **Training**    | 12,544             | 12,542         | 204,283 | 2,614             | 2                      |
| **Development** | 2,001              | 2,001          | 25,147  | 359               | 0                      |
| **Test**        | 2,077              | 2,077          | 25,094  | 354               | 0                      |

### Key Statistics:

- **17 Universal POS tags** found: ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X
- **Vocabulary size**: 19,656 unique words across all datasets
- **Average sentence length**: ~14 words
- **Maximum sentence length**: 104 words (after filtering)

## ğŸš€ Quick Start

### 1. Basic Usage

```python
from conllu_processor import CoNLLUProcessor

# Load and process a single file
processor = CoNLLUProcessor()
processor.load_conllu_file("data/ud_english_ewt/en_ewt-ud-dev.conllu")

# Get clean (word, POS) pairs
sentences = processor.get_word_pos_pairs()
print(f"Loaded {len(sentences)} sentences")
```

### 2. Load All Datasets

```python
from conllu_processor import load_ud_english_data

# Load all three datasets at once
processors = load_ud_english_data(
    "data/ud_english_ewt/en_ewt-ud-train.conllu",
    "data/ud_english_ewt/en_ewt-ud-dev.conllu",
    "data/ud_english_ewt/en_ewt-ud-test.conllu"
)

# Access individual datasets
train_data = processors['train'].get_word_pos_pairs()
dev_data = processors['dev'].get_word_pos_pairs()
test_data = processors['test'].get_word_pos_pairs()
```

### 3. Run Tests

```bash
# Test the implementation
python test_processor.py

# See usage examples
python usage_example.py
```

## ğŸ“ File Structure

```
â”œâ”€â”€ conllu_processor.py       # Main implementation
â”œâ”€â”€ test_processor.py         # Comprehensive test suite
â”œâ”€â”€ usage_example.py          # Usage examples and demonstrations
â”œâ”€â”€ README_implementation.md  # This documentation
â””â”€â”€ data/
    â””â”€â”€ ud_english_ewt/
        â”œâ”€â”€ en_ewt-ud-train.conllu
        â”œâ”€â”€ en_ewt-ud-dev.conllu
        â””â”€â”€ en_ewt-ud-test.conllu
```

## ğŸ”§ Implementation Details

### CoNLLUProcessor Class

The main `CoNLLUProcessor` class provides:

#### Key Methods:

- `load_conllu_file(filepath, max_sentence_length=128)`: Load and process a .conllu file
- `get_word_pos_pairs()`: Returns list of sentences with (word, UPOS) pairs
- `get_vocabulary()`: Get word frequency dictionary
- `get_pos_tags()`: Get POS tag frequency dictionary
- `print_statistics()`: Display processing statistics

#### Data Processing Features:

1. **Multiword Token Handling**: Automatically detects and removes lines like "19-20 don't"
2. **Empty Node Filtering**: Removes syntactic empty nodes (decimal IDs)
3. **Sentence Length Filtering**: Removes sentences longer than 128 words
4. **Robust Error Handling**: Continues processing even with malformed lines
5. **Detailed Statistics**: Tracks all processing steps and provides comprehensive stats

### Data Format

The processed data is returned as:

```python
# List of sentences
sentences = [
    [("Google", "PROPN"), ("is", "AUX"), ("a", "DET"), ("nice", "ADJ"), ("search", "NOUN"), ("engine", "NOUN"), (".", "PUNCT")],
    [("What", "PRON"), ("if", "SCONJ"), ("Google", "PROPN"), ("Morphed", "VERB"), ("Into", "ADP"), ("GoogleOS", "PROPN"), ("?", "PUNCT")],
    # ... more sentences
]
```

## ğŸ§ª Testing and Validation

The implementation includes comprehensive testing:

### Test Suite Features:

- âœ… **Multiword token detection** validation
- âœ… **Data loading** from all three files
- âœ… **Statistics generation** verification
- âœ… **Sample data inspection** for correctness
- âœ… **Error handling** for missing files

### Run All Tests:

```bash
python test_processor.py
```

Expected output: "ALL TESTS PASSED! âœ“"

## ğŸ“ˆ Data Quality Validation

The implementation ensures high data quality:

1. **Format Validation**: Each line must have exactly 10 tab-separated columns
2. **Content Validation**: Words and POS tags cannot be empty or "\_"
3. **Structure Validation**: Proper sentence boundaries and metadata handling
4. **Length Validation**: Sentences > 128 words are filtered out
5. **Consistency Validation**: All 17 standard Universal POS tags are preserved

## ğŸ”œ Next Steps for Neural Model

Your data is now ready for the neural PoS tagging model implementation:

1. **Tokenization**: Use Keras Tokenizer to convert words to IDs
2. **Padding**: Pad sequences to uniform length for batch processing
3. **Label Encoding**: Convert POS tags to numerical labels
4. **Model Architecture**: Implement LSTM-based sequence labeling
5. **Training**: Train on the processed training data
6. **Evaluation**: Evaluate on dev/test sets

## ğŸ› Troubleshooting

### Common Issues:

**FileNotFoundError**: Make sure data files are in the correct location:

```
data/ud_english_ewt/en_ewt-ud-train.conllu
data/ud_english_ewt/en_ewt-ud-dev.conllu
data/ud_english_ewt/en_ewt-ud-test.conllu
```

**Import Errors**: Make sure all files are in the same directory:

```python
from conllu_processor import CoNLLUProcessor
```

**Memory Issues**: If processing large files, consider processing in batches or using a machine with more RAM.

## ğŸ“š Technical References

- **Universal Dependencies**: https://universaldependencies.org/
- **CoNLL-U Format**: https://universaldependencies.org/format.html
- **UD English EWT Treebank**: https://github.com/UniversalDependencies/UD_English-EWT

## ğŸ“ Assignment Context

This implementation completes **Part 1** of your NLU Lab 1 assignment:

- âœ… Process UD English treebank data
- âœ… Remove multiword tokens and empty lines
- âœ… Filter sentences by length (max 128 words)
- âœ… Extract clean (word, UPOS) pairs for model training

**Next**: Implement the neural PoS tagging model using this processed data.

---

**Status**: Part 1 Complete âœ…
